[
  {
    "source": "NetUnicorn Basic Client Usage",
    "content": "To interact with NetUnicorn, you first need to create a RemoteClient instance. Example: \nfrom netunicorn.client.remote import RemoteClient\nclient = RemoteClient(endpoint='your_endpoint', login='your_login', password='your_password')\nclient.healthcheck() can be used to verify the connection."
  },
  {
    "source": "NetUnicorn Experiment Object and Environment Definition",
    "content": "An Experiment in NetUnicorn is defined by creating an `Experiment` object and then mapping a `Pipeline` to a set of nodes. Example: \nfrom netunicorn.base.experiment import Experiment\nfrom netunicorn.base.pipeline import Pipeline\n# Assume 'pipeline' is a defined Pipeline object and 'working_nodes' is a list of Node objects\nexperiment = Experiment()\nexperiment.map(pipeline, working_nodes) \nSetting `experiment.environment_definition` (e.g., to `ShellExecution`) is often not required if tasks are self-contained or use standard library functionalities, as a default environment is typically used. Only set it explicitly if specific environment behavior (like a Docker image or a plain shell for raw commands) is needed. If setting, ensure the class (e.g., `from netunicorn.base.environment_definitions import ShellExecution`) is imported and set BEFORE `experiment.map()`."
  },
  {
    "source": "NetUnicorn Pipeline Basics and Chaining Tasks",
    "content": "A Pipeline defines a sequence of tasks. Tasks are added using .then(). For multi-step operations, chain appropriate tasks. \nExample of creating a file and then uploading it:\nfrom netunicorn.base.pipeline import Pipeline\nfrom netunicorn.library.tasks.flags import ExecuteShellCommand\nfrom netunicorn.library.tasks.upload.fileio import UploadToFileIO\n\npipeline = Pipeline().then(\n    ExecuteShellCommand(command=\"echo 'Sample content for upload' > upload_me.txt\")\n).then(\n    UploadToFileIO(filepath=\"upload_me.txt\", expires=\"1d\")\n)\nEnsure all task classes like `ExecuteShellCommand` and `UploadToFileIO` are imported. For such pipeline, `experiment.environment_definition = ShellExecution()` would be appropriate."
  },
  {
    "source": "NetUnicorn Node Selection",
    "content": "To get nodes, use `client.get_nodes()`. This returns a `NodePool` object. You can filter nodes or take a subset. Example: \nnode_pool = client.get_nodes()\n# To use a specific number of available nodes:\nworking_nodes = node_pool.take(1) # Takes the first available node\n# To filter for a specific node by name:\n# working_nodes = node_pool.filter(lambda node: node.name == 'snl-server-5').take(1)\nEnsure `working_nodes` is a list-like structure. Explicitly importing `from netunicorn.base.nodes import NodePool` is generally NOT required for type hinting or using methods like `.take()` or `.filter()`."
  },
  {
    "source": "NetUnicorn Environment Definitions",
    "content": "Experiments MAY require an `environment_definition` if the default execution environment is not suitable. This is assigned to `experiment.environment_definition`. \n- `ShellExecution`: Explicitly use for tasks that are simple shell commands (e.g. `ExecuteShellCommand(command='ls')`) or Python tasks that require a basic shell. Import with `from netunicorn.base.environment_definitions import ShellExecution`. Example: `experiment.environment_definition = ShellExecution()`\n- `DockerImage`: For tasks that need to run within a specific Docker container. Import with `from netunicorn.base.environment_definitions import DockerImage`. Example: `experiment.environment_definition = DockerImage(image='your_docker_image:latest')`\nMany library tasks (e.g. `OoklaSpeedtest`, `SleepTask`) run correctly without an explicitly set `environment_definition`. If set, it must be done on the `Experiment` object before calling `experiment.map()`."
  },
  {
    "source": "NetUnicorn Experiment Lifecycle, Naming, and Preparation",
    "content": "Define an `experiment_name` (or `experiment_label`) as a string. This can be a simple static string (e.g., 'my_test_experiment') or a dynamically generated unique string (e.g., using timestamps) if running many experiments where names might collide. \nPython Example for unique name (optional, use if needed):\n```python\nimport datetime\nexperiment_name = f\"my_dynamic_experiment_{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S%f\")}\"\n```\nBefore preparing, it's good practice to attempt to delete any pre-existing experiment with the same name:\n```python\nfrom netunicorn.client.remote import RemoteClientException # Ensure RemoteClientException is imported\n# client = RemoteClient(...) # Assume client is initialized and experiment_name is defined\ntry:\n    client.delete_experiment(experiment_name)\nexcept RemoteClientException:\n    pass # Common to ignore if experiment simply doesn't exist\n```\nThen, prepare, start, and manage the experiment using the `Experiment` object:\n# Assume 'experiment' is an Experiment object and 'experiment_name' is defined.\nclient.prepare_experiment(experiment, experiment_name) # Pass the Experiment object here!\nImport `ExperimentStatus` from `netunicorn.base.experiment` for polling."
  },
  {
    "source": "NetUnicorn SleepTask",
    "content": "The SleepTask makes the node sleep for a specified number of seconds. Example: \nfrom netunicorn.library.tasks.basic import SleepTask\ntask = SleepTask(duration_in_seconds=10) # e.g., SleepTask(10) for 10 seconds."
  },
  {
    "source": "NetUnicorn Full Script Structure Example (Simplified Polling)",
    "content": "A typical NetUnicorn script involves: \n1. **Imports**: `RemoteClient`, `RemoteClientException` (from `netunicorn.client.remote`), `Pipeline` (from `netunicorn.base.pipeline`), `Experiment`, `ExperimentStatus` (from `netunicorn.base.experiment`), task-specific classes. Standard libraries like `time`. `ShellExecution` if explicitly needed. \n2. **Credentials**: Define NetUnicorn connection parameters. \n3. **Client Initialization**: `client = RemoteClient(...)`. \n4. **Pipeline Creation**: `pipeline = Pipeline().then(TaskA())`. \n5. **Node Selection**: `node_pool = client.get_nodes()`, then `working_nodes = node_pool.take(1)`. \n6. **Experiment Object Creation & Mapping**: `experiment = Experiment().map(pipeline, working_nodes)`. (Set `environment_definition` before map only if needed). \n7. **Experiment Name**: `experiment_name = 'my_simple_experiment'`. \n8. **Delete Old Experiment**: `try-except RemoteClientException` block for `client.delete_experiment(experiment_name)`. \n9. **Prepare Experiment**: `client.prepare_experiment(experiment, experiment_name)`. \n10. **Poll for READY**: Loop with `info = client.get_experiment_status(experiment_name)`, check `info.status == ExperimentStatus.READY`. Import `time` for `time.sleep()`. \n11. **Start Experiment**: `client.start_execution(experiment_name)`. \n12. **Poll for Completion**: Loop with `info = client.get_experiment_status(experiment_name)`, check `info.status != ExperimentStatus.RUNNING`. \n13. **Retrieve and Print Results**: `final_status_info = client.get_experiment_status(experiment_name)`. Process `final_status_info.execution_result`. \nExample Snippet (Focus on Lifecycle):\n```python\n# 1. Imports (adapt as needed)\nfrom netunicorn.client.remote import RemoteClient, RemoteClientException\nfrom netunicorn.base.pipeline import Pipeline\nfrom netunicorn.base.experiment import Experiment, ExperimentStatus\nfrom netunicorn.library.tasks.basic import SleepTask # Example task\nimport time\nimport os\n\n# 2. Credentials (example assumes env vars or direct assignment)\nNETUNICORN_ENDPOINT = os.getenv('NETUNICORN_ENDPOINT', 'YOUR_ENDPOINT')\nNETUNICORN_LOGIN = os.getenv('NETUNICORN_LOGIN', 'YOUR_LOGIN')\nNETUNICORN_PASSWORD = os.getenv('NETUNICORN_PASSWORD', 'YOUR_PASSWORD')\n\n# 3. Client Initialization\nclient = RemoteClient(endpoint=NETUNICORN_ENDPOINT, login=NETUNICORN_LOGIN, password=NETUNICORN_PASSWORD)\nclient.healthcheck()\n\n# 4. Pipeline Creation\npipeline = Pipeline().then(SleepTask(5))\n\n# 5. Node Selection\nnode_pool = client.get_nodes()\nif not node_pool:\n    print(\"No nodes available, exiting.\")\n    exit()\nworking_nodes = node_pool.take(1)\nif not working_nodes:\n    print(\"Failed to take any nodes, exiting.\")\n    exit()\nprint(f\"Selected nodes: {working_nodes}\")\n\n# 6. Experiment Object Creation & Mapping\nexperiment = Experiment().map(pipeline, working_nodes)\n# experiment.environment_definition = ShellExecution() # Only if needed!\n\n# 7. Experiment Name\nexperiment_name = f\"simple_example_script_{time.time()}\" # Static or simple dynamic\nprint(f\"Using experiment name: {experiment_name}\")\n\n# 8. Delete Old Experiment\ntry:\n    client.delete_experiment(experiment_name)\n    print(f\"Successfully deleted pre-existing experiment: {experiment_name}\")\nexcept RemoteClientException as e:\n    print(f\"Info: Could not delete experiment {experiment_name} (may not exist or already deleted): {e}\")\n\n# 9. Prepare Experiment\nprint(f\"Preparing experiment: {experiment_name}\" )\nclient.prepare_experiment(experiment, experiment_name)\n\n# 10. Poll for READY\nprint(f\"Waiting for experiment {experiment_name} to be READY...\")\nwhile True:\n    info = client.get_experiment_status(experiment_name)\n    status = info.status\n    print(f\"Current status of {experiment_name}: {status}\")\n    if status == ExperimentStatus.READY:\n        print(f\"Experiment {experiment_name} is READY.\")\n        break\n    # Simplified error check: if it's finished or errored before ready\n    elif status == ExperimentStatus.FINISHED or status == ExperimentStatus.ERROR: # Still good to check for terminal states here\n        print(f\"Experiment {experiment_name} reached terminal state {status} before becoming READY. Error: {info.error}\")\n        exit()\n    time.sleep(10)\n\n# 11. Start Experiment\nprint(f\"Starting execution of {experiment_name}...\")\nclient.start_execution(experiment_name)\n\n# 12. Poll for Completion (until not RUNNING)\nprint(f\"Waiting for experiment {experiment_name} to complete...\")\nwhile True:\n    info = client.get_experiment_status(experiment_name)\n    status = info.status\n    print(f\"Current status of {experiment_name}: {status}\")\n    if status != ExperimentStatus.RUNNING and status != ExperimentStatus.PREPARING: # PREPARING can sometimes be seen if polling very fast after start\n        print(f\"Experiment {experiment_name} is no longer RUNNING. Final status: {status}\")\n        break\n    time.sleep(20)\n\n# 13. Retrieve and Print Results\nprint(f\"Retrieving results for {experiment_name}...\")\nfinal_status_info = client.get_experiment_status(experiment_name)\nprint(f\"Final status: {final_status_info.status}\")\nif final_status_info.error:\n    print(f\"Experiment Error: {final_status_info.error}\")\n\nif final_status_info.execution_result:\n    results = final_status_info.execution_result\n    print(f\"Execution Result for {experiment_name}:\")\n    for report_list in results: # Assuming results is list of lists of reports\n        for report in report_list:\n            if report:\n                print(f\"  Node: {report.node.name}, Success: {report.success}\")\n                # print(f\"    Log: {report.log}\") # Can be very verbose\n                if not report.success:\n                    print(f\"    Error details: {report.error}\")\nelse:\n    print(f\"No execution_result content found for {experiment_name}.\")\n\nprint(f\"Script for {experiment_name} concluded.\")\n```"
  },
  {
    "source": "NetUnicorn BaseClient Methods",
    "content": "The `netunicorn.client.base.BaseClient` provides core methods. Key methods include:\n- `get_nodes()`: Returns currently available nodes as a `NodePool` object (from `netunicorn.base.nodes`).\n- `delete_experiment(experiment_name: str)`: Deletes an experiment. Raises `RemoteClientException` if experiment doesn't exist or other server-side issues occur.\n- `healthcheck()`: Checks server health.\n- `prepare_experiment(experiment: Experiment, experiment_id: str)`: Prepares an `Experiment` object for execution under the given `experiment_id` string.\n- `start_execution(experiment_id: str)`: Starts a prepared experiment.\n- `get_experiment_status(experiment_id: str)`: Returns `ExperimentExecutionInformation` (from `netunicorn.base.experiment`) which includes status and results. This method is used for polling the experiment's progress.\n(Full reference: https://netunicorn.github.io/netunicorn/_autosummary/netunicorn.client.base.BaseClient.html)"
  },
  {
    "source": "NetUnicorn Library: basic.py Tasks",
    "content": "Common tasks from `netunicorn.library.tasks.basic`. Import specific tasks like `from netunicorn.library.tasks.basic import SleepTask`.\n- `DummyTask()`: Simple task, `run()` returns `True`. For testing.\n- `SleepTask(seconds: int)`: Pauses execution for `seconds`. Example: `SleepTask(10)`."
  },
  {
    "source": "NetUnicorn Library: flags.py Tasks",
    "content": "Tasks from `netunicorn.library.tasks.flags` for node interaction. Import specific tasks e.g., `from netunicorn.library.tasks.flags import ExecuteShellCommand`.\n- `ExecuteShellCommand(command: str)`: Executes a shell `command` on the node. Example: `ExecuteShellCommand(command=\"echo 'Hello' > /tmp/output.txt\")`. Returns `Success` (with stdout) or `Failure` (with stderr). Requires `experiment.environment_definition = ShellExecution()` to be set."
  },
  {
    "source": "NetUnicorn Library: upload.fileio Task",
    "content": "Task from `netunicorn.library.tasks.upload.fileio` for uploading files to file.io. Import with `from netunicorn.library.tasks.upload.fileio import UploadToFileIO`.\n- `UploadToFileIO(filepath: str, expires: str = \"14d\")`: Uploads the local file at `filepath` (on the node) to `file.io`. `expires` (e.g., \"1d\", \"2h\") controls expiration. Example: `UploadToFileIO(filepath=\"my_data.zip\", expires=\"7d\")`."
  },
  {
    "source": "NetUnicorn Library: tasks_utils.py Functions",
    "content": "Utility functions from `netunicorn.library.tasks.tasks_utils` (primarily for use within other tasks):\n- `subprocess_run(arguments: list[str]) -> Result`: A helper function to run a subprocess with the given `arguments` (a list of strings). It captures stdout and stderr. Returns `Success(stdout_lines)` or `Failure(stderr_lines)`."
  },
  {
    "source": "NetUnicorn Library: capture Tasks",
    "content": "Tasks from `netunicorn.library.tasks.capture` for network traffic capture:\n- `capture.tcpdump.StartCapture(filepath: str, arguments: Optional[List[str]] = None)`: Starts a tcpdump process, saving output to `filepath`. Custom `arguments` can be provided.\n- `capture.tcpdump.StopNamedCapture(name: str)`: Stops a tcpdump process that was started with a specific `name` (referring to the `StartCapture` task name).\n- `capture.tcpdump.StopAllCapture()`: Stops all currently running tcpdump processes initiated by the library on the node.\n- `capture.tshark.StartCapture(filepath: str, arguments: Optional[List[str]] = None)`: Starts a tshark process, saving output to `filepath`. Custom `arguments` can be provided.\n- `capture.tshark.StopNamedCapture(name: str)`: Stops a tshark process started with a specific `name`.\n- `capture.tshark.StopAllCapture()`: Stops all currently running tshark processes initiated by the library on the node."
  },
  {
    "source": "NetUnicorn Library: data_transfer Tasks",
    "content": "Tasks from `netunicorn.library.tasks.data_transfer` for moving files between nodes after task execution:\n- `data_transfer.SendData(filepath: str, task_name: str, data_type: Literal[\"ookla-speedtest\", \"pcp-speedtest\", \"iperf3\", \"netperf\", \"flent\", \"file\"] = \"file\", local_filepath_is_temporary: bool = False)`: Makes the file at `filepath` available for fetching by other nodes. `task_name` is used to identify this data source. `data_type` provides context. If `local_filepath_is_temporary` is true, the file might be deleted after being fetched.\n- `data_transfer.FetchData(send_data_task: str, endpoint: str)`: Fetches a file that was made available by a `SendData` task (identified by `send_data_task` which is the name of the SendData task in the pipeline) from the specified `endpoint` (node name or IP of the node that ran SendData). The fetched file is typically stored in the current working directory of the FetchData task."
  },
  {
    "source": "NetUnicorn Library: letsencrypt Tasks",
    "content": "Tasks from `netunicorn.library.tasks.letsencrypt` for automating Let's Encrypt certificate issuance:\n- `LetsEncryptDNS01Validation(acme_server: str, email: str, domains: List[str], dns_hook: str, dns_unhook: str, key_type: str = \"rsa\")`: Performs DNS-01 validation. Requires providing paths to `dns_hook` and `dns_unhook` scripts compatible with dehydrated.\n- `LetsEncryptHTTP01Validation(acme_server: str, email: str, domains: List[str], key_type: str = \"rsa\")`: Performs HTTP-01 validation. The node must be publicly accessible on port 80.\nHelper functions (usually not called directly in pipelines):\n- `validate_http_01(domain: str, token_name: str, token_data: str)`\n- `validate_dns_01(domain: str, validation_string: str)`"
  },
  {
    "source": "NetUnicorn Library: measurements Tasks (General)",
    "content": "General measurement tasks from `netunicorn.library.tasks.measurements`:\n- `measurements.flent.StartServer()`: Starts a flent (Flexible Network Tester) server process.\n- `measurements.flent.StopServer(start_server_task_name: str)`: Stops a flent server started by a previous task named `start_server_task_name`.\n- `measurements.flent.RunFlentTest(server_address: str, test_name: str, plot_filename: Optional[str] = None, test_parameters: Optional[dict] = None)`: Runs a flent test against `server_address` using `test_name` (e.g., 'tcp_good'). Optionally saves a plot and passes extra parameters.\n- `measurements.iperf3.Iperf3ServerStart(flags: Optional[list[str]] = None)`: Starts an iperf3 server with optional `flags`.\n- `measurements.iperf3.Iperf3ServerStop(server_task_name: str)`: Stops an iperf3 server started by `server_task_name`.\n- `measurements.iperf3.Iperf3Client(server_ip: str, flags: Optional[list[str]] = None)`: Runs an iperf3 client against `server_ip` with optional `flags`.\n- `measurements.ndt.NDT7SpeedTest(server: Optional[str] = None)`: Performs an NDT7 speed test, optionally specifying a server. Also available: `NDT7SpeedTestLinuxAMD64`, `NDT7SpeedTestLinuxARM64`. `_NDTSpeedTestImplementation` is a base class.\n- `measurements.ookla_speedtest.OoklaSpeedtest(server_selection_task_name: str = \"\", source_ip: str = \"\", timeout: int = 100)`: Runs an Ookla Speedtest CLI test. Can take a server ID from a `ServerSelection` task result (via `server_selection_task_name`) or ping a `source_ip` to select a server. If neither, selects automatically.\n- `measurements.ookla_speedtest.ServerSelection(callback: Callable[[list[ServerInfo]], str])`: Lists available Ookla servers and allows a user-provided `callback` function to choose one by returning its ID. `ServerInfo` is a dataclass with id, host, port, name, location, country.\n- `measurements.ookla_speedtest.OoklaSpeedtestAnalysis(speedtest_task_name: str)`: Analyzes the JSON results from a previous `OoklaSpeedtest` task (identified by `speedtest_task_name`). It classifies latency (good, ok, strange, problem) and throughput (low, ok, good, excellent) and returns a summary dictionary.\n- `measurements.ping.Ping(host: str, count: Optional[int] = None, interval: Optional[float] = None, timeout: Optional[float] = None, interface: Optional[str] = None)`: Performs an ICMP ping to `host` with options for count, interval, timeout, and source interface."
  },
  {
    "source": "NetUnicorn Library: measurements Tasks (Specialized)",
    "content": "Specialized measurement tasks from submodules of `netunicorn.library.tasks.measurements`:\n- `measurements.alexa.alexa.AlexaWebsitesTask(num_of_websites: int)`: Performs network measurements (curl, dig, traceroute, ping) to the top `num_of_websites` from the Alexa Top 1 Million list.\n- `measurements.cloudflare.speedtest.CloudflareSpeedTest(count: int = 3, warmup_bytes: int = 100000)`: Runs a speed test using Cloudflare's network, performing `count` tests after a warmup phase. (Dispatcher for `CloudflareSpeedTestLinuxImplementation`)"
  },
  {
    "source": "NetUnicorn Library: network_attacks Tasks",
    "content": "Tasks from `netunicorn.library.tasks.network_attacks` and its submodules, for simulating various network attacks. Use responsibly and only on authorized systems.\n- `network_attacks.arp.ArpSpoof(target_ip: str, spoof_ip: str, interface: str = \"eth0\", duration_seconds: int = 60)`: Performs ARP spoofing between `target_ip` and `spoof_ip` on the given `interface` for `duration_seconds`.\n- `network_attacks.cve202141773.CVE202141773(target: str, path: str = \"/cgi-bin/.%2e/%2e%2e/%2e%2e/etc/passwd\")`: Attempts to exploit Apache path traversal (CVE-2021-41773) against `target` using the specified `path`.\n- `network_attacks.ftp.BruteForceFTP(target: str, username: str, wordlist: list[str])`: Attempts to brute-force FTP login credentials.\n- `network_attacks.heartbleed.Heartbleed(host: str, port: int = 443, count: int = 1, length: int = 0xFFFF, tls_version: Literal[\"tls10\", \"tls11\", \"tls12\"] = \"tls11\", starttls_proto: Optional[str] = None, data: Optional[str] = None)`: Tests for the Heartbleed vulnerability (CVE-2014-0160).\n- `network_attacks.heartbleed2.Heartbleed(IPaddress: str, port: int, starttls: bool = False, debug: bool = False)`: An alternative implementation for testing Heartbleed.\n- `network_attacks.icmp.ICMPRedirection(target: str, old_gw: str, new_gw: str)`: Sends ICMP redirect packets to `target`, instructing it to use `new_gw` instead of `old_gw`.\n- `network_attacks.land.LANDAttack(target_ip: str, source_port: int = 1001, destination_port: int = 80)`: Performs a LAND attack by sending a packet with the same source and destination IP/port to `target_ip`.\n- `network_attacks.log4j.CVE202144228(cc_address: str, hosts: list[str])`: Attempts to trigger the Log4Shell vulnerability (CVE-2021-44228) by sending a crafted User-Agent to `hosts`, pointing to a JNDI lookup at `cc_address`.\n- `network_attacks.loris.SlowLoris(host: str, port: int = 80, sockets: int = 150, https: bool = False, sleeptime: int = 15, slowloris_iterations: int = 100)`: Performs a Slowloris denial-of-service attack.\n- `network_attacks.loris.SMBLoris(host: str, starting_source_port: int = 10000, number_of_ports: int = 1000)`: Performs a resource exhaustion attack against SMB (port 445).\n- `network_attacks.mac.MACFlooder(iface: str = \"eth0\", count: int = 1000)`: Floods the network with Ethernet frames with random MAC addresses to overwhelm switch CAM tables.\n- `network_attacks.mail.FakeMail(host: str, port: int, sender: str, recipient: str, subject: str, body: str)`: Sends an email using raw SMTP commands (potentially for spoofing, use with caution).\n- `network_attacks.ssh.BruteForceSSH(targetIP: str, wordlist: list[str], port: int = 22, user: str = \"root\")`: Attempts to brute-force SSH login credentials using paramiko."
  },
  {
    "source": "NetUnicorn Library: preprocessing Tasks",
    "content": "Tasks from `netunicorn.library.tasks.preprocessing` for processing captured network data (e.g., pcap files) on the node:\n- `preprocessing.scapy.Get5Tuples(filename: str)`: Reads a pcap file (`filename`) and extracts 5-tuples (src_ip, dst_ip, src_port, dst_port, proto) for TCP/UDP packets.\n- `preprocessing.scapy.GetDNSQueries(filename: str)`: Extracts DNS query names from a pcap file.\n- `preprocessing.scapy.GetHTTPHostHeaders(filename: str)`: Attempts to extract HTTP Host headers from raw packet data in a pcap file.\n- `preprocessing.scapy.GetICMPRequests(filename: str)`: Extracts ICMP echo request packets from a pcap file.\n- `preprocessing.scapy.GetUniqueARPMAC(filename: str)`: Extracts unique source MAC addresses from ARP packets in a pcap file.\n- `preprocessing.tshark.TsharkCommand(command: list[str])`: Executes a given `tshark` command (provided as a list of strings) on the node.\n- `preprocessing.zeek.ZeekPCAPAnalysis(pcap_filename: str, flags: Optional[list[str]] = None)`: Analyzes the `pcap_filename` using Zeek with optional `flags`. This is a TaskDispatcher for `ZeekPCAPAnalysisLinuxImplementation` which includes requirements to install Zeek from a Debian repository."
  },
  {
    "source": "NetUnicorn Library: qoe_youtube Tasks",
    "content": "Tasks from `netunicorn.library.tasks.qoe_youtube` for Quality of Experience (QoE) monitoring of YouTube videos. This system involves a collection server and a video watching task.\n- `qoe_youtube.StartQoECollectionServer(data_folder: str = \".\", interface: str = \"0.0.0.0\", port: int = 34543)`: Starts a FastAPI-based server (`qoe_collector.py`) on the node to receive QoE data. Data is stored in `data_folder`. The server listens on `interface:port`. Returns `(success_message_with_pid, process_id)` or `Failure`. (Dispatcher for Linux implementation with uvicorn requirements).\n- `qoe_youtube.StopQoECollectionServer(start_task_name: str)`: Stops the QoE collection server that was started by the task named `start_task_name`.\n- `qoe_youtube.WatchYouTubeVideo(video_url: str, duration: Optional[int] = None, quality: Optional[int] = None, qoe_server_address: str = \"localhost\", qoe_server_port: int = 34543, report_time: int = 250)`: Watches a YouTube `video_url` using Selenium and two Chrome extensions (an adblocker and a custom QoE stats collector). It sends QoE metrics (player state, quality changes, periodic stats) to the specified `qoe_server_address:qoe_server_port`. `duration` specifies watch time (None for full video). `quality` can request a specific resolution. `report_time` is the interval for sending periodic stats (in ms). (Dispatcher for Linux implementation with extensive Selenium/Chrome/extension setup requirements). The underlying watcher logic is in `watcher.py`."
  },
  {
    "source": "NetUnicorn Library: upload Tasks (General Info)",
    "content": "General information about upload tasks. For specific tasks like `UploadToFileIO`, see their dedicated entries. Tasks in `netunicorn.library.tasks.upload` facilitate uploading files from nodes.\nOther tasks include:\n- `upload.ftp.UploadToFTP(...)`: Uploads to an FTP server.\n- `upload.googlecloud.UploadToGoogleCloudStorage(...)`: Uploads to Google Cloud Storage.\n- `upload.webdav.UploadToWebDav(...)`: Uploads to WebDAV."
  },
  {
    "source": "NetUnicorn Library: utils Tasks",
    "content": "General utility tasks from `netunicorn.library.tasks.utils`:\n- `utils.network.PortKnock(ip: str, port: int)`: Attempts a single TCP connection (a \"knock\") to the specified `ip` and `port`. Does not return success/failure of the knock itself, always returns 0.\n- `utils.sleep.RandomSleepTask(seconds_min: int, seconds_max: int)`: A TaskDispatcher that returns a `SleepTask` (from `basic.py`) which will sleep for a random duration between `seconds_min` and `seconds_max` (inclusive)."
  },
  {
    "source": "NetUnicorn Library: video_watchers Tasks",
    "content": "Selenium-based tasks from `netunicorn.library.tasks.video_watchers` for watching online videos. These are simpler watchers compared to the `qoe_youtube` one and do not involve QoE metric collection extensions by default.\n- `video_watchers.twitch_watcher.WatchTwitchStream(video_url: str, duration: Optional[int] = 10, chrome_location: Optional[str] = None, webdriver_arguments: Optional[list] = None)`: Watches a Twitch stream at `video_url` for `duration` seconds. Uses a headless Chrome/Chromium environment via Xvfb.\n- `video_watchers.vimeo_watcher.WatchVimeoVideo(video_url: str, duration: Optional[int] = 100, chrome_location: Optional[str] = None, webdriver_arguments: Optional[list] = None)`: Watches a Vimeo video at `video_url`. If `duration` is provided, watches for that many seconds. If `duration` is None, it attempts to watch until the video ends by checking player state.\n- `video_watchers.youtube_watcher.WatchYouTubeVideo(video_url: str, duration: Optional[int] = 100, chrome_location: Optional[str] = None, webdriver_arguments: Optional[list] = None)`: Watches a YouTube video at `video_url`. Similar to the Vimeo watcher, it can watch for a fixed `duration` or attempt to watch until the end by checking player state via JavaScript. Uses `YouTubeIFrameStatus` enum for player states.\nAll these watchers are TaskDispatchers that delegate to Linux-specific implementations which handle Selenium and Xvfb setup (typically requiring `chromium`, `chromium-driver`, `xvfb`, `selenium`). The default `chrome_location` is often `/usr/bin/chromium`."
  },
  {
    "source": "NetUnicorn Library: Example Pipelines (measurements)",
    "content": "The `netunicorn.library.pipelines.measurements` module provides example pre-defined pipelines:\n- `pipelines.measurements.ookla_speedtest.simple_speedtest_pipeline() -> Pipeline`: Defines a pipeline that runs `OoklaSpeedtest` and then `OoklaSpeedtestAnalysis` on its results. A good example of a simple measurement and analysis sequence.\n- `pipelines.measurements.netflex_pipeline.netflex_ookla_full_loop_pipeline() -> Pipeline`: Runs `OoklaSpeedtest`, then uses `SendData` to send results to an endpoint defined by the `RAG_ENDPOINT` environment variable, and then uses `FetchData` to retrieve data from the same endpoint. This suggests an integration pattern with an external RAG/analysis system.\n- `pipelines.measurements.netflex_pipeline.netflex_mlab_full_loop_pipeline() -> Pipeline`: Similar to the Ookla loop, but runs `NDT7SpeedTest` (with JSON format), then `SendData` (type \"mlab-speedtest\") to `RAG_ENDPOINT`, and `FetchData` from `RAG_ENDPOINT`. Note: `NDT7SpeedTest` is used but not explicitly imported in this file, relying on its availability.\nThese example pipelines demonstrate how to chain measurement tasks, data transfer tasks, and analysis tasks, and show potential integration patterns with external systems via environment variables like `RAG_ENDPOINT`."
  }
]