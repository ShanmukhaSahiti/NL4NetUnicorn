[
  {
    "source": "NetUnicorn Basic Client Usage",
    "content": "To interact with NetUnicorn, you first need to create a RemoteClient instance. Example: \nfrom netunicorn.client.remote import RemoteClient\nclient = RemoteClient(endpoint='your_endpoint', login='your_login', password='your_password')\nclient.healthcheck() can be used to verify the connection."
  },
  {
    "source": "NetUnicorn Experiment Object and Environment Definition",
    "content": "An Experiment in NetUnicorn is defined by creating an `Experiment` object, setting its `environment_definition`, and then mapping a `Pipeline` to a set of nodes. \nExample: \nfrom netunicorn.base.experiment import Experiment\nfrom netunicorn.base.pipeline import Pipeline\nfrom netunicorn.base.environment_definitions import ShellExecution # Or DockerImage, etc.\n# Assume 'pipeline' is a defined Pipeline object and 'working_nodes' is a list of Node objects\nexperiment = Experiment()\nexperiment.environment_definition = ShellExecution() # Specify how tasks will run. MUST be set before map().\nexperiment.map(pipeline, working_nodes) \nCrucially, `experiment.environment_definition` must be set BEFORE `experiment.map()`."
  },
  {
    "source": "NetUnicorn Pipeline Basics and Chaining Tasks",
    "content": "A Pipeline defines a sequence of tasks. Tasks are added using .then(). For multi-step operations, chain appropriate tasks. \nExample of creating a file and then uploading it:\nfrom netunicorn.base.pipeline import Pipeline\nfrom netunicorn.library.tasks.flags import ExecuteShellCommand\nfrom netunicorn.library.tasks.upload.fileio import UploadToFileIO\n\npipeline = Pipeline().then(\n    ExecuteShellCommand(command=\\\"echo 'Sample content for upload' > upload_me.txt\\\")\n).then(\n    UploadToFileIO(filepath=\\\"upload_me.txt\\\", expires=\\\"1d\\\")\n)\nEnsure all task classes like `ExecuteShellCommand` and `UploadToFileIO` are imported."
  },
  {
    "source": "NetUnicorn Node Selection",
    "content": "To get nodes, use `client.get_nodes()`. This returns a `NodePool` object. You can filter nodes or take a subset. Example: \nfrom netunicorn.base.nodes import NodePool # Optional if only using methods like .take()\nnode_pool: NodePool = client.get_nodes()\n# To use a specific number of available nodes:\nworking_nodes = node_pool.take(1) # Takes the first available node\n# To filter for a specific node by name:\n# working_nodes = node_pool.filter(lambda node: node.name == 'snl-server-5').take(1)\nEnsure `working_nodes` is a list-like structure (result of `.take(N)` or a list of `Node` objects) suitable for `experiment.map()`."
  },
  {
    "source": "NetUnicorn Environment Definitions",
    "content": "Experiments require an `environment_definition` to specify how tasks should be executed. This is assigned to `experiment.environment_definition`. Common choices include:\n- `ShellExecution`: For tasks that are simple shell commands or Python tasks/scripts not requiring special docker environments. Import with `from netunicorn.base.environment_definitions import ShellExecution`. Example: `experiment.environment_definition = ShellExecution()`\n- `DockerImage`: For tasks that need to run within a specific Docker container. Import with `from netunicorn.base.environment_definitions import DockerImage`. Example: `experiment.environment_definition = DockerImage(image='your_docker_image:latest')`\nAlways set `experiment.environment_definition` on the `Experiment` object before calling `experiment.map()`. Ensure the chosen class (e.g., `ShellExecution`) is imported."
  },
  {
    "source": "NetUnicorn Experiment Lifecycle, Naming, and Preparation",
    "content": "Define a unique string for `experiment_name`. It is crucial to make this name unique for each run, e.g., by including a timestamp or UUID. \nPython Example for unique name:\n```python\nimport datetime\nimport uuid\n# Using timestamp (recommended for readability):\ntimestamp = datetime.datetime.now().strftime(\\\"%Y%m%d%H%M%S\\\")\nexperiment_name = f\\\"my_dynamic_experiment_{timestamp}\\\"\\n# Or using UUID:\n# experiment_name = f\\\"my_experiment_{uuid.uuid4().hex[:8]}\\\"\\n```\nBefore preparing, always attempt to delete any pre-existing experiment with the same name to avoid conflicts:\n```python\nfrom netunicorn.client.remote import RemoteClientException # Ensure RemoteClientException is imported\n# client = RemoteClient(...) # Assume client is initialized and experiment_name is defined\ntry:\\n    client.delete_experiment(experiment_name)\\n    print(f\\\"Successfully deleted pre-existing experiment: {experiment_name}\\\")\\nexcept RemoteClientException as e:\\n    print(f\\\"Info: Could not delete experiment {experiment_name} (may not exist or already deleted): {e}\\\")\\nexcept Exception as e: # Catch any other potential exceptions\\n    print(f\\\"Warning: An unexpected error occurred while trying to delete {experiment_name}: {e}\\\" )\\n```\nThen, prepare, start, and manage the experiment using the `Experiment` object:\n# Assume 'experiment' is an Experiment object (already defined with environment and mapped with pipeline/nodes) and 'experiment_name' is defined.\nclient.prepare_experiment(experiment, experiment_name) # Pass the Experiment object here!\n# ... (polling for READY, start, polling for FINISHED, get results - see Full Script Example) ...\nImport `ExperimentStatus` from `netunicorn.base.experiment` for polling."
  },
  {
    "source": "NetUnicorn SleepTask",
    "content": "The SleepTask makes the node sleep for a specified number of seconds. Example: \nfrom netunicorn.library.tasks.basic import SleepTask\ntask = SleepTask(duration_in_seconds=10) # e.g., SleepTask(10) for 10 seconds."
  },
  {
    "source": "NetUnicorn Full Script Structure Example",
    "content": "A typical NetUnicorn script involves: \n1. **Imports**: `RemoteClient`, `RemoteClientException` (from `netunicorn.client.remote`), `Pipeline` (from `netunicorn.base.pipeline`), `Experiment`, `ExperimentStatus` (from `netunicorn.base.experiment`), `ShellExecution` or other environment definitions (from `netunicorn.base.environment_definitions`), task-specific classes (e.g., `SleepTask`, `ExecuteShellCommand`, `UploadToFileIO` from their respective `netunicorn.library.tasks.*` paths), and standard libraries like `time`, `datetime`, `uuid`. \n2. **Credentials**: Define NetUnicorn connection parameters (endpoint, login, password) - these will be injected by the RAG system. \n3. **Client Initialization**: `client = RemoteClient(endpoint=..., login=..., password=...)`. \n4. **Pipeline Creation**: `pipeline = Pipeline().then(TaskA()).then(TaskB())`. \n5. **Node Selection**: `node_pool = client.get_nodes()`, then `working_nodes = node_pool.take(1)` (or filter). \n6. **Experiment Object Creation**: `experiment = Experiment()`. \n7. **Environment Definition**: `experiment.environment_definition = ShellExecution()`. \n8. **Experiment Mapping**: `experiment.map(pipeline, working_nodes)`. \n9. **Unique Experiment Name**: Generate `experiment_name` using `datetime` or `uuid` (see 'Experiment Lifecycle, Naming, and Preparation' entry). \n10. **Delete Old Experiment**: `try-except RemoteClientException` block for `client.delete_experiment(experiment_name)`. \n11. **Prepare Experiment**: `client.prepare_experiment(experiment, experiment_name)`. \n12. **Poll for READY**: Loop with `client.get_experiment_status(experiment_name).status`, waiting for `ExperimentStatus.READY`. Import `time` for `time.sleep()`. \n13. **Start Experiment**: `client.start_execution(experiment_name)`. \n14. **Poll for FINISHED**: Loop with `client.get_experiment_status(experiment_name).status`, waiting for `ExperimentStatus.FINISHED` or `ExperimentStatus.ERROR`. \n15. **Retrieve and Print Results**: `final_status_info = client.get_experiment_status(experiment_name)`. If `final_status_info.status == ExperimentStatus.FINISHED`, print `final_status_info.execution_result`. Iterate through results for detailed reports. \nComprehensive Example Snippet:\n```python\n# 1. Imports (ensure all are listed)\nfrom netunicorn.client.remote import RemoteClient, RemoteClientException\nfrom netunicorn.base.pipeline import Pipeline\nfrom netunicorn.base.experiment import Experiment, ExperimentStatus\nfrom netunicorn.base.environment_definitions import ShellExecution # Example environment\nfrom netunicorn.library.tasks.flags import ExecuteShellCommand # Example task\nfrom netunicorn.library.tasks.upload.fileio import UploadToFileIO # Example task\nimport time\nimport datetime\nimport os # For credentials from env vars, if not hardcoded by RAG\n\n# 2. Credentials (RAG should inject these; example shows direct assignment)\nNETUNICORN_ENDPOINT = os.getenv('NETUNICORN_ENDPOINT', 'YOUR_ENDPOINT')\nNETUNICORN_LOGIN = os.getenv('NETUNICORN_LOGIN', 'YOUR_LOGIN')\nNETUNICORN_PASSWORD = os.getenv('NETUNICORN_PASSWORD', 'YOUR_PASSWORD')\n\n# 3. Client Initialization\nclient = RemoteClient(endpoint=NETUNICORN_ENDPOINT, login=NETUNICORN_LOGIN, password=NETUNICORN_PASSWORD)\nclient.healthcheck()\n\n# 4. Pipeline Creation\npipeline = Pipeline().then(\n    ExecuteShellCommand(command=\\\"echo 'Hello from RAG! This is a test file.' > test_file.txt\\\")\n).then(\n    UploadToFileIO(filepath=\\\"test_file.txt\\\", expires=\\\"1h\\\")\n)\n\n# 5. Node Selection\nnode_pool = client.get_nodes()\nif not node_pool: \n    print(\\\"No nodes available, exiting.\\\")\n    exit()\nworking_nodes = node_pool.take(1)\nif not working_nodes: \n    print(\\\"Failed to take any nodes, exiting.\\\")\n    exit()\nprint(f\\\"Selected nodes: {working_nodes}\\\")\n\n# 6. Experiment Object Creation\nexperiment = Experiment()\n\n# 7. Environment Definition\nexperiment.environment_definition = ShellExecution()\n\n# 8. Experiment Mapping\nexperiment.map(pipeline, working_nodes)\n\n# 9. Unique Experiment Name\nexperiment_name = f\\\"rag_script_example_{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')}\\\"\nprint(f\\\"Using experiment name: {experiment_name}\\\")\n\n# 10. Delete Old Experiment\ntry:\\n    client.delete_experiment(experiment_name)\\n    print(f\\\"Successfully deleted pre-existing experiment: {experiment_name}\\\")\\nexcept RemoteClientException as e:\\n    print(f\\\"Info: Could not delete experiment {experiment_name} (may not exist or already deleted): {e}\\\")\\nexcept Exception as e:\\n    print(f\\\"Warning: An unexpected error occurred while trying to delete experiment {experiment_name}: {e}\\\" )\n\n# 11. Prepare Experiment\nprint(f\\\"Preparing experiment: {experiment_name}\\\" )\nclient.prepare_experiment(experiment, experiment_name)\n\n# 12. Poll for READY\nprint(f\\\"Waiting for experiment {experiment_name} to be READY...\\\")\nwhile True:\\n    try:\\n        status_info = client.get_experiment_status(experiment_name)\\n        status = status_info.status\\n        print(f\\\"Current status of {experiment_name}: {status}\\\")\\n        if status == ExperimentStatus.READY:\\n            print(f\\\"Experiment {experiment_name} is READY.\\\")\\n            break\\n        elif status in [ExperimentStatus.FINISHED, ExperimentStatus.ERROR]:\\n            print(f\\\"Experiment {experiment_name} reached terminal state {status} before becoming READY. Error: {status_info.error}\\\")\\n            exit()\\n    except Exception as e:\\n        print(f\\\"Error polling status for {experiment_name}: {e}. Retrying...\\\")\\n    time.sleep(10) # Poll every 10 seconds\n\n# 13. Start Experiment\nprint(f\\\"Starting execution of {experiment_name}...\\\")\nclient.start_execution(experiment_name)\n\n# 14. Poll for FINISHED\nprint(f\\\"Waiting for experiment {experiment_name} to FINISH...\\\")\nwhile True:\\n    try:\\n        status_info = client.get_experiment_status(experiment_name)\\n        status = status_info.status\\n        print(f\\\"Current status of {experiment_name}: {status}\\\")\\n        if status == ExperimentStatus.FINISHED:\\n            print(f\\\"Experiment {experiment_name} FINISHED successfully.\\\")\\n            break\\n        elif status == ExperimentStatus.ERROR:\\n            print(f\\\"Experiment {experiment_name} entered ERROR state. Error: {status_info.error}\\\")\\n            break \\n    except Exception as e:\\n        print(f\\\"Error polling status for {experiment_name}: {e}. Retrying...\\\")\\n    time.sleep(20) # Poll every 20 seconds\n\n# 15. Retrieve and Print Results\nprint(f\\\"Retrieving results for {experiment_name}...\\\")\nfinal_status_info = client.get_experiment_status(experiment_name)\nif final_status_info.status == ExperimentStatus.FINISHED:\n    results = final_status_info.execution_result\n    print(f\\\"Execution Result for {experiment_name}: {results}\\\")\\n    if results:\n        for i, report_list in enumerate(results):\n            print(f\\\"  Results from node pool {i}:\\\")\\n            if report_list:\n                 for j, report in enumerate(report_list):\n                    if report: # Report can be None if a node fails\n                        print(f\\\"    Report {j} on node {report.node.name}: Success={report.success}, Log Length={len(report.log) if report.log else 0}\\\")\n                        # print(f\\\"      Log: {report.log}\\\" ) # Potentially very long\n                        if not report.success:\n                            print(f\\\"      Error: {report.error}\\\" )\n                    else:\n                        print(f\\\"    Report {j} is None.\\\")\n            else:\n                print(f\\\"  Node pool {i} has no reports (list is None or empty).\\\")\n    else:\n        print(f\\\"No execution_result content found for {experiment_name}, though status is FINISHED.\\\")\nelse:\n    print(f\\\"Experiment {experiment_name} did not finish successfully. Final status: {final_status_info.status}. Error: {final_status_info.error}\\\")\n\nprint(f\\\"Script for {experiment_name} concluded.\\\")\n```"
  },
  {
    "source": "NetUnicorn BaseClient Methods",
    "content": "The `netunicorn.client.base.BaseClient` provides core methods. Key methods include:\n- `get_nodes()`: Returns currently available nodes as a `NodePool` object (from `netunicorn.base.nodes`).\n- `delete_experiment(experiment_name: str)`: Deletes an experiment. Raises `RemoteClientException` if experiment doesn't exist or other server-side issues occur.\n- `healthcheck()`: Checks server health.\n- `prepare_experiment(experiment: Experiment, experiment_id: str)`: Prepares an `Experiment` object for execution under the given `experiment_id` string.\n- `start_execution(experiment_id: str)`: Starts a prepared experiment.\n- `get_experiment_status(experiment_id: str)`: Returns `ExperimentExecutionInformation` (from `netunicorn.base.experiment`) which includes status and results. This method is used for polling the experiment's progress (e.g., to check for READY or FINISHED states).\n(Full reference: https://netunicorn.github.io/netunicorn/_autosummary/netunicorn.client.base.BaseClient.html)"
  },
  {
    "source": "NetUnicorn Library: basic.py Tasks",
    "content": "Common tasks from `netunicorn.library.tasks.basic`. Import specific tasks like `from netunicorn.library.tasks.basic import SleepTask`.\n- `DummyTask()`: Simple task, `run()` returns `True`. For testing.\n- `SleepTask(seconds: int)`: Pauses execution for `seconds`. Example: `SleepTask(10)`."
  },
  {
    "source": "NetUnicorn Library: flags.py Tasks",
    "content": "Tasks from `netunicorn.library.tasks.flags` for node interaction. Import specific tasks e.g., `from netunicorn.library.tasks.flags import ExecuteShellCommand`.\n- `ExecuteShellCommand(command: str)`: Executes a shell `command` on the node. Example: `ExecuteShellCommand(command=\\\"echo 'Hello' > /tmp/output.txt\\\")`. Returns `Success` (with stdout) or `Failure` (with stderr)."
  },
  {
    "source": "NetUnicorn Library: upload.fileio Task",
    "content": "Task from `netunicorn.library.tasks.upload.fileio` for uploading files to file.io. Import with `from netunicorn.library.tasks.upload.fileio import UploadToFileIO`.\n- `UploadToFileIO(filepath: str, expires: str = \\\"14d\\\")`: Uploads the local file at `filepath` (on the node) to `file.io`. `expires` (e.g., \\\"1d\\\", \\\"2h\\\") controls expiration. Example: `UploadToFileIO(filepath=\\\"my_data.zip\\\", expires=\\\"7d\\\")`."
  },
  {
    "source": "NetUnicorn Library: tasks_utils.py Functions",
    "content": "Utility functions from `netunicorn.library.tasks.tasks_utils` (primarily for use within other tasks):\n- `subprocess_run(arguments: list[str]) -> Result`: A helper function to run a subprocess with the given `arguments` (a list of strings). It captures stdout and stderr. Returns `Success(stdout_lines)` or `Failure(stderr_lines)`."
  },
  {
    "source": "NetUnicorn Library: capture Tasks",
    "content": "Tasks from `netunicorn.library.tasks.capture` for network traffic capture:\n- `capture.tcpdump.StartCapture(filepath: str, arguments: Optional[List[str]] = None)`: Starts a tcpdump process, saving output to `filepath`. Custom `arguments` can be provided.\n- `capture.tcpdump.StopNamedCapture(name: str)`: Stops a tcpdump process that was started with a specific `name` (referring to the `StartCapture` task name).\n- `capture.tcpdump.StopAllCapture()`: Stops all currently running tcpdump processes initiated by the library on the node.\n- `capture.tshark.StartCapture(filepath: str, arguments: Optional[List[str]] = None)`: Starts a tshark process, saving output to `filepath`. Custom `arguments` can be provided.\n- `capture.tshark.StopNamedCapture(name: str)`: Stops a tshark process started with a specific `name`.\n- `capture.tshark.StopAllCapture()`: Stops all currently running tshark processes initiated by the library on the node."
  },
  {
    "source": "NetUnicorn Library: data_transfer Tasks",
    "content": "Tasks from `netunicorn.library.tasks.data_transfer` for moving files between nodes after task execution:\n- `data_transfer.SendData(filepath: str, task_name: str, data_type: Literal[\\\"ookla-speedtest\\\", \\\"pcp-speedtest\\\", \\\"iperf3\\\", \\\"netperf\\\", \\\"flent\\\", \\\"file\\\"] = \\\"file\\\", local_filepath_is_temporary: bool = False)`: Makes the file at `filepath` available for fetching by other nodes. `task_name` is used to identify this data source. `data_type` provides context. If `local_filepath_is_temporary` is true, the file might be deleted after being fetched.\n- `data_transfer.FetchData(send_data_task: str, endpoint: str)`: Fetches a file that was made available by a `SendData` task (identified by `send_data_task` which is the name of the SendData task in the pipeline) from the specified `endpoint` (node name or IP of the node that ran SendData). The fetched file is typically stored in the current working directory of the FetchData task."
  },
  {
    "source": "NetUnicorn Library: letsencrypt Tasks",
    "content": "Tasks from `netunicorn.library.tasks.letsencrypt` for automating Let's Encrypt certificate issuance:\n- `LetsEncryptDNS01Validation(acme_server: str, email: str, domains: List[str], dns_hook: str, dns_unhook: str, key_type: str = \\\"rsa\\\")`: Performs DNS-01 validation. Requires providing paths to `dns_hook` and `dns_unhook` scripts compatible with dehydrated.\n- `LetsEncryptHTTP01Validation(acme_server: str, email: str, domains: List[str], key_type: str = \\\"rsa\\\")`: Performs HTTP-01 validation. The node must be publicly accessible on port 80.\nHelper functions (usually not called directly in pipelines):\n- `validate_http_01(domain: str, token_name: str, token_data: str)`\n- `validate_dns_01(domain: str, validation_string: str)`"
  },
  {
    "source": "NetUnicorn Library: measurements Tasks (General)",
    "content": "General measurement tasks from `netunicorn.library.tasks.measurements`:\n- `measurements.flent.StartServer()`: Starts a flent (Flexible Network Tester) server process.\n- `measurements.flent.StopServer(start_server_task_name: str)`: Stops a flent server started by a previous task named `start_server_task_name`.\n- `measurements.flent.RunFlentTest(server_address: str, test_name: str, plot_filename: Optional[str] = None, test_parameters: Optional[dict] = None)`: Runs a flent test against `server_address` using `test_name` (e.g., 'tcp_good'). Optionally saves a plot and passes extra parameters.\n- `measurements.iperf3.Iperf3ServerStart(flags: Optional[list[str]] = None)`: Starts an iperf3 server with optional `flags`.\n- `measurements.iperf3.Iperf3ServerStop(server_task_name: str)`: Stops an iperf3 server started by `server_task_name`.\n- `measurements.iperf3.Iperf3Client(server_ip: str, flags: Optional[list[str]] = None)`: Runs an iperf3 client against `server_ip` with optional `flags`.\n- `measurements.ndt.NDT7SpeedTest(server: Optional[str] = None)`: Performs an NDT7 speed test, optionally specifying a server. Also available: `NDT7SpeedTestLinuxAMD64`, `NDT7SpeedTestLinuxARM64`. `_NDTSpeedTestImplementation` is a base class.\n- `measurements.ookla_speedtest.OoklaSpeedtest(server_selection_task_name: str = \\\"\\\", source_ip: str = \\\"\\\", timeout: int = 100)`: Runs an Ookla Speedtest CLI test. Can take a server ID from a `ServerSelection` task result (via `server_selection_task_name`) or ping a `source_ip` to select a server. If neither, selects automatically.\n- `measurements.ookla_speedtest.ServerSelection(callback: Callable[[list[ServerInfo]], str])`: Lists available Ookla servers and allows a user-provided `callback` function to choose one by returning its ID. `ServerInfo` is a dataclass with id, host, port, name, location, country.\n- `measurements.ookla_speedtest.OoklaSpeedtestAnalysis(speedtest_task_name: str)`: Analyzes the JSON results from a previous `OoklaSpeedtest` task (identified by `speedtest_task_name`). It classifies latency (good, ok, strange, problem) and throughput (low, ok, good, excellent) and returns a summary dictionary.\n- `measurements.ping.Ping(host: str, count: Optional[int] = None, interval: Optional[float] = None, timeout: Optional[float] = None, interface: Optional[str] = None)`: Performs an ICMP ping to `host` with options for count, interval, timeout, and source interface."
  },
  {
    "source": "NetUnicorn Library: measurements Tasks (Specialized)",
    "content": "Specialized measurement tasks from submodules of `netunicorn.library.tasks.measurements`:\n- `measurements.alexa.alexa.AlexaWebsitesTask(num_of_websites: int)`: Performs network measurements (curl, dig, traceroute, ping) to the top `num_of_websites` from the Alexa Top 1 Million list.\n- `measurements.cloudflare.speedtest.CloudflareSpeedTest(count: int = 3, warmup_bytes: int = 100000)`: Runs a speed test using Cloudflare's network, performing `count` tests after a warmup phase. (Dispatcher for `CloudflareSpeedTestLinuxImplementation`)"
  },
  {
    "source": "NetUnicorn Library: network_attacks Tasks",
    "content": "Tasks from `netunicorn.library.tasks.network_attacks` and its submodules, for simulating various network attacks. Use responsibly and only on authorized systems.\n- `network_attacks.arp.ArpSpoof(target_ip: str, spoof_ip: str, interface: str = \\\"eth0\\\", duration_seconds: int = 60)`: Performs ARP spoofing between `target_ip` and `spoof_ip` on the given `interface` for `duration_seconds`.\n- `network_attacks.cve202141773.CVE202141773(target: str, path: str = \\\"/cgi-bin/.%2e/%2e%2e/%2e%2e/etc/passwd\\\")`: Attempts to exploit Apache path traversal (CVE-2021-41773) against `target` using the specified `path`.\n- `network_attacks.ftp.BruteForceFTP(target: str, username: str, wordlist: list[str])`: Attempts to brute-force FTP login credentials.\n- `network_attacks.heartbleed.Heartbleed(host: str, port: int = 443, count: int = 1, length: int = 0xFFFF, tls_version: Literal[\\\"tls10\\\", \\\"tls11\\\", \\\"tls12\\\"] = \\\"tls11\\\", starttls_proto: Optional[str] = None, data: Optional[str] = None)`: Tests for the Heartbleed vulnerability (CVE-2014-0160).\n- `network_attacks.heartbleed2.Heartbleed(IPaddress: str, port: int, starttls: bool = False, debug: bool = False)`: An alternative implementation for testing Heartbleed.\n- `network_attacks.icmp.ICMPRedirection(target: str, old_gw: str, new_gw: str)`: Sends ICMP redirect packets to `target`, instructing it to use `new_gw` instead of `old_gw`.\n- `network_attacks.land.LANDAttack(target_ip: str, source_port: int = 1001, destination_port: int = 80)`: Performs a LAND attack by sending a packet with the same source and destination IP/port to `target_ip`.\n- `network_attacks.log4j.CVE202144228(cc_address: str, hosts: list[str])`: Attempts to trigger the Log4Shell vulnerability (CVE-2021-44228) by sending a crafted User-Agent to `hosts`, pointing to a JNDI lookup at `cc_address`.\n- `network_attacks.loris.SlowLoris(host: str, port: int = 80, sockets: int = 150, https: bool = False, sleeptime: int = 15, slowloris_iterations: int = 100)`: Performs a Slowloris denial-of-service attack.\n- `network_attacks.loris.SMBLoris(host: str, starting_source_port: int = 10000, number_of_ports: int = 1000)`: Performs a resource exhaustion attack against SMB (port 445).\n- `network_attacks.mac.MACFlooder(iface: str = \\\"eth0\\\", count: int = 1000)`: Floods the network with Ethernet frames with random MAC addresses to overwhelm switch CAM tables.\n- `network_attacks.mail.FakeMail(host: str, port: int, sender: str, recipient: str, subject: str, body: str)`: Sends an email using raw SMTP commands (potentially for spoofing, use with caution).\n- `network_attacks.ssh.BruteForceSSH(targetIP: str, wordlist: list[str], port: int = 22, user: str = \\\"root\\\")`: Attempts to brute-force SSH login credentials using paramiko."
  },
  {
    "source": "NetUnicorn Library: preprocessing Tasks",
    "content": "Tasks from `netunicorn.library.tasks.preprocessing` for processing captured network data (e.g., pcap files) on the node:\n- `preprocessing.scapy.Get5Tuples(filename: str)`: Reads a pcap file (`filename`) and extracts 5-tuples (src_ip, dst_ip, src_port, dst_port, proto) for TCP/UDP packets.\n- `preprocessing.scapy.GetDNSQueries(filename: str)`: Extracts DNS query names from a pcap file.\n- `preprocessing.scapy.GetHTTPHostHeaders(filename: str)`: Attempts to extract HTTP Host headers from raw packet data in a pcap file.\n- `preprocessing.scapy.GetICMPRequests(filename: str)`: Extracts ICMP echo request packets from a pcap file.\n- `preprocessing.scapy.GetUniqueARPMAC(filename: str)`: Extracts unique source MAC addresses from ARP packets in a pcap file.\n- `preprocessing.tshark.TsharkCommand(command: list[str])`: Executes a given `tshark` command (provided as a list of strings) on the node.\n- `preprocessing.zeek.ZeekPCAPAnalysis(pcap_filename: str, flags: Optional[list[str]] = None)`: Analyzes the `pcap_filename` using Zeek with optional `flags`. This is a TaskDispatcher for `ZeekPCAPAnalysisLinuxImplementation` which includes requirements to install Zeek from a Debian repository."
  },
  {
    "source": "NetUnicorn Library: qoe_youtube Tasks",
    "content": "Tasks from `netunicorn.library.tasks.qoe_youtube` for Quality of Experience (QoE) monitoring of YouTube videos. This system involves a collection server and a video watching task.\n- `qoe_youtube.StartQoECollectionServer(data_folder: str = \\\".\\\", interface: str = \\\"0.0.0.0\\\", port: int = 34543)`: Starts a FastAPI-based server (`qoe_collector.py`) on the node to receive QoE data. Data is stored in `data_folder`. The server listens on `interface:port`. Returns `(success_message_with_pid, process_id)` or `Failure`. (Dispatcher for Linux implementation with uvicorn requirements).\n- `qoe_youtube.StopQoECollectionServer(start_task_name: str)`: Stops the QoE collection server that was started by the task named `start_task_name`.\n- `qoe_youtube.WatchYouTubeVideo(video_url: str, duration: Optional[int] = None, quality: Optional[int] = None, qoe_server_address: str = \\\"localhost\\\", qoe_server_port: int = 34543, report_time: int = 250)`: Watches a YouTube `video_url` using Selenium and two Chrome extensions (an adblocker and a custom QoE stats collector). It sends QoE metrics (player state, quality changes, periodic stats) to the specified `qoe_server_address:qoe_server_port`. `duration` specifies watch time (None for full video). `quality` can request a specific resolution. `report_time` is the interval for sending periodic stats (in ms). (Dispatcher for Linux implementation with extensive Selenium/Chrome/extension setup requirements). The underlying watcher logic is in `watcher.py`."
  },
  {
    "source": "NetUnicorn Library: upload Tasks (General Info)",
    "content": "General information about upload tasks. For specific tasks like `UploadToFileIO`, see their dedicated entries. Tasks in `netunicorn.library.tasks.upload` facilitate uploading files from nodes.\nOther tasks include:\n- `upload.ftp.UploadToFTP(...)`: Uploads to an FTP server.\n- `upload.googlecloud.UploadToGoogleCloudStorage(...)`: Uploads to Google Cloud Storage.\n- `upload.webdav.UploadToWebDav(...)`: Uploads to WebDAV."
  },
  {
    "source": "NetUnicorn Library: utils Tasks",
    "content": "General utility tasks from `netunicorn.library.tasks.utils`:\n- `utils.network.PortKnock(ip: str, port: int)`: Attempts a single TCP connection (a \\\"knock\\\") to the specified `ip` and `port`. Does not return success/failure of the knock itself, always returns 0.\n- `utils.sleep.RandomSleepTask(seconds_min: int, seconds_max: int)`: A TaskDispatcher that returns a `SleepTask` (from `basic.py`) which will sleep for a random duration between `seconds_min` and `seconds_max` (inclusive)."
  },
  {
    "source": "NetUnicorn Library: video_watchers Tasks",
    "content": "Selenium-based tasks from `netunicorn.library.tasks.video_watchers` for watching online videos. These are simpler watchers compared to the `qoe_youtube` one and do not involve QoE metric collection extensions by default.\n- `video_watchers.twitch_watcher.WatchTwitchStream(video_url: str, duration: Optional[int] = 10, chrome_location: Optional[str] = None, webdriver_arguments: Optional[list] = None)`: Watches a Twitch stream at `video_url` for `duration` seconds. Uses a headless Chrome/Chromium environment via Xvfb.\n- `video_watchers.vimeo_watcher.WatchVimeoVideo(video_url: str, duration: Optional[int] = 100, chrome_location: Optional[str] = None, webdriver_arguments: Optional[list] = None)`: Watches a Vimeo video at `video_url`. If `duration` is provided, watches for that many seconds. If `duration` is None, it attempts to watch until the video ends by checking player state.\n- `video_watchers.youtube_watcher.WatchYouTubeVideo(video_url: str, duration: Optional[int] = 100, chrome_location: Optional[str] = None, webdriver_arguments: Optional[list] = None)`: Watches a YouTube video at `video_url`. Similar to the Vimeo watcher, it can watch for a fixed `duration` or attempt to watch until the end by checking player state via JavaScript. Uses `YouTubeIFrameStatus` enum for player states.\nAll these watchers are TaskDispatchers that delegate to Linux-specific implementations which handle Selenium and Xvfb setup (typically requiring `chromium`, `chromium-driver`, `xvfb`, `selenium`). The default `chrome_location` is often `/usr/bin/chromium`."
  },
  {
    "source": "NetUnicorn Library: Example Pipelines (measurements)",
    "content": "The `netunicorn.library.pipelines.measurements` module provides example pre-defined pipelines:\n- `pipelines.measurements.ookla_speedtest.simple_speedtest_pipeline() -> Pipeline`: Defines a pipeline that runs `OoklaSpeedtest` and then `OoklaSpeedtestAnalysis` on its results. A good example of a simple measurement and analysis sequence.\n- `pipelines.measurements.netflex_pipeline.netflex_ookla_full_loop_pipeline() -> Pipeline`: Runs `OoklaSpeedtest`, then uses `SendData` to send results to an endpoint defined by the `RAG_ENDPOINT` environment variable, and then uses `FetchData` to retrieve data from the same endpoint. This suggests an integration pattern with an external RAG/analysis system.\n- `pipelines.measurements.netflex_pipeline.netflex_mlab_full_loop_pipeline() -> Pipeline`: Similar to the Ookla loop, but runs `NDT7SpeedTest` (with JSON format), then `SendData` (type \\\"mlab-speedtest\\\") to `RAG_ENDPOINT`, and `FetchData` from `RAG_ENDPOINT`. Note: `NDT7SpeedTest` is used but not explicitly imported in this file, relying on its availability.\nThese example pipelines demonstrate how to chain measurement tasks, data transfer tasks, and analysis tasks, and show potential integration patterns with external systems via environment variables like `RAG_ENDPOINT`."
  }
]